<!DOCTYPE html>
<html lang="en">
  <head>
    <title>Statistics: fitcknn</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css" integrity="sha512-1ycn6IcaQQ40/MKBW2W4Rhis/DbILU74C1vSrLJxCq57o941Ym01SwNsOMqvEBFlcgUa6xLiPY/NS5R+E6ztJQ==" crossorigin="anonymous" referrerpolicy="no-referrer">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/css/bootstrap.min.css" integrity="sha384-KyZXEAg3QhqLMpG8r+8fhAXLRk2vvoC2f3B09zVXn8CA5QIVfZOJ3BCsw2P0p/We" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/js/bootstrap.bundle.min.js" integrity="sha384-U1DAWAznBHeqEIlVSCgzq+c9gqGAJn5c/t99JyeKa9xxaYpSvHU5awsuZVVFIhvj" crossorigin="anonymous"></script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">
    </script>
    <style>
    var {
      font-style: italics;
      font-weight: bold;
    }
    td {
      vertical-align: top;
    }
    </style>
  </head>
  <body>
    <div class="bg-dark">
      <div class="container-xl">
        <nav class="navbar navbar-expand-lg navbar-dark bg-dark">
          <div class="container-fluid">
            <a class="navbar-brand" href=index.html>
              <img src="assets/statistics.png" alt="statistics" class="d-inline-block align-top" width="25" height="25">
              Statistics
            </a>
            <button type="button" class="navbar-toggler" data-bs-toggle="collapse" data-bs-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
              <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
              <ul class="navbar-nav">
                <li class="nav-item">
                  <a class="nav-link" href="index.html#Model Fitting">
                    <i class="fas fa-list-alt"></i>
                    Model Fitting
                  </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="https://gnu-octave.github.io/packages/">
                  <img src="assets/octave-logo.svg" alt="GNU Octave logo" class="d-inline-block align-top" width="25" height="25">
                    Octave Packages
                  </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="https://www.octave.org">
                    <i class="fas fa-home"></i>
                    GNU Octave website
                  </a>
                </li>
              </ul>
            </div>
          </div>
        </nav>
      </div>
    </div>
    <div class="container-xl my-4">
      <div class="card rounded">
        <div class="card-header card-header-mod">
          <div class="row d-flex flex-wrap align-items-center">
            <div class="col-sm-3 col-md-5 mb-2 mb-sm-0">
              <h3 class="d-inline-block mr-2">
              Function&nbsp;Reference: <b><code>fitcknn</code></b>
              </h3>
            </div>
          </div>
        </div>
        <div class="card-body">
<dl>
<dt><u>statistics:</u> <var>obj</var> = <b>fitcknn</b><i> (<var>X</var>, <var>Y</var>)</i></dt>
<dt><u>statistics:</u> <var>obj</var> = <b>fitcknn</b><i> (&hellip;, <var>name</var>, <var>value</var>)</i></dt>
</dl>

<p> Fit a k-Nearest Neighbor classification model.
</p><div class="ms-5">

<p> <code><var>obj</var> = fitcknn (<var>X</var>, <var>Y</var>)</code> returns a k-Nearest Neighbor
 classification model, <var>obj</var>, with <var>X</var> being the predictor data,
 and <var>Y</var> the class labels of observations in <var>X</var>.
</p>
 <ul>
<li>
 <code>X</code> must be a <math>N&times;P</math> numeric matrix of input data where rows
 correspond to observations and columns correspond to features or variables.
 <var>X</var> will be used to train the kNN model.
 </li><li>
 <code>Y</code> is <math>N&times;1</math> matrix or cell matrix containing the class labels of
 corresponding predictor data in <var>X</var>. <var>Y</var> can contain any type of
 categorical data. <var>Y</var> must have same numbers of Rows as <var>X</var>.
 </li></ul>


<p> <code><var>obj</var> = fitcknn (&hellip;, <var>name</var>, <var>value</var>)</code> returns a
 k-Nearest Neighbor classification model with additional options specified by
 <code>Name-Value</code> pair arguments listed below.
</p>
 <table>
<thead><tr><th width="18%"></th><th width="2%"><var>Name</var></th><th width="80%"><var>Value</var></th></tr></thead>
<tr><td width="18%"><code>&quot;PredictorNames&quot;</code></td><td width="2%"></td><td width="80%">A cell array of character vectors
 specifying the predictor variable names.  The variable names are assumed to
 be in the same order as they appear in the training data <var>X</var>.</td></tr>
<tr><td width="18%"><code>&quot;ResponseName&quot;</code></td><td width="2%"></td><td width="80%">A character vector specifying the name
 of the response variable.</td></tr>
<tr><td width="18%"><code>&quot;ClassNames&quot;</code></td><td width="2%"></td><td width="80%">A cell array of character vectors
 specifying the names of the classes in the training data <var>Y</var>.</td></tr>
<tr><td width="18%"><code>&quot;BreakTies&quot;</code></td><td width="2%"></td><td width="80%">Tie-breaking algorithm used by predict
 when multiple classes have the same smallest cost. By default, ties occur
 when multiple classes have the same number of nearest points among the
 <math>k</math> nearest neighbors. The available options are specified by the
 following character arrays:</td></tr>
</table>


 <table>
<tr><td width="5%"></td><td width="20%"><code>&quot;smallest&quot;</code></td><td width="75%">This is the default and it favors the
 class with the smallest index among the tied groups, i.e. the one that
 appears first in the training labelled data.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;nearest&quot;</code></td><td width="75%">This favors the class with the nearest
 neighbor among the tied groups, i.e. the class with the closest member point
 according to the distance metric used.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;nearest&quot;</code></td><td width="75%">This randomly picks one class among the
 tied groups.</td></tr>
</table>


 <table>
<tr><td width="18%"><code>&quot;BucketSize&quot;</code></td><td width="2%"></td><td width="80%">The maximum number of data points in the
 leaf node of the Kd-tree and it must be a positive integer.  By default, it
 is 50. This argument is meaningful only when the selected search method is
 <code>&quot;kdtree&quot;</code>.</td></tr>
<tr><td width="18%"><code>&quot;Cost&quot;</code></td><td width="2%"></td><td width="80%">A <math>N&times;R</math> numeric matrix containing
 misclassification cost for the corresponding instances in <var>X</var> where
 <math>R</math> is the number of unique categories in <var>Y</var>.  If an instance is
 correctly classified into its category the cost is calculated to be 1, If
 not then 0. cost matrix can be altered use <code><var>obj.cost</var> = somecost</code>.
 default value <code><var>cost</var> = ones(rows(X),numel(unique(Y)))</code>.</td></tr>
<tr><td width="18%"><code>&quot;Prior&quot;</code></td><td width="2%"></td><td width="80%">A numeric vector specifying the prior
 probabilities for each class.  The order of the elements in <code>Prior</code>
 corresponds to the order of the classes in <code>ClassNames</code>.</td></tr>
<tr><td width="18%"><code>&quot;NumNeighbors&quot;</code></td><td width="2%"></td><td width="80%">A positive integer value specifying
 the number of nearest neighbors to be found in the kNN search.  By default,
 it is 1.</td></tr>
<tr><td width="18%"><code>&quot;Exponent&quot;</code></td><td width="2%"></td><td width="80%">A positive scalar (usually an integer)
 specifying the Minkowski distance exponent.  This argument is only valid when
 the selected distance metric is <code>&quot;minkowski&quot;</code>.  By default it is 2.</td></tr>
<tr><td width="18%"><code>&quot;Scale&quot;</code></td><td width="2%"></td><td width="80%">A nonnegative numeric vector specifying the
 scale parameters for the standardized Euclidean distance.  The vector length
 must be equal to the number of columns in <var>X</var>.  This argument is only
 valid when the selected distance metric is <code>&quot;seuclidean&quot;</code>, in which
 case each coordinate of <var>X</var> is scaled by the corresponding element of
 <code>&quot;scale&quot;</code>, as is each query point in <var>Y</var>.  By default, the scale
 parameter is the standard deviation of each coordinate in <var>X</var>.  If a
 variable in <var>X</var> is constant, i.e. zero variance, this value is forced
 to 1 to avoid division by zero.  This is the equivalent of this variable not
 being standardized.</td></tr>
<tr><td width="18%"><code>&quot;Cov&quot;</code></td><td width="2%"></td><td width="80%">A square matrix with the same number of columns
 as <var>X</var> specifying the covariance matrix for computing the mahalanobis
 distance.  This must be a positive definite matrix matching.  This argument
 is only valid when the selected distance metric is <code>&quot;mahalanobis&quot;</code>.</td></tr>
<tr><td width="18%"><code>&quot;Distance&quot;</code></td><td width="2%"></td><td width="80%">is the distance metric used by
 <code>knnsearch</code> as specified below:</td></tr>
</table>


 <table>
<tr><td width="5%"></td><td width="20%"><code>&quot;euclidean&quot;</code></td><td width="75%">Euclidean distance.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;seuclidean&quot;</code></td><td width="75%">standardized Euclidean distance.  Each
 coordinate difference between the rows in <var>X</var> and the query matrix
 <var>Y</var> is scaled by dividing by the corresponding element of the standard
 deviation computed from <var>X</var>.  To specify a different scaling, use the
 <code>&quot;Scale&quot;</code> name-value argument.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;cityblock&quot;</code></td><td width="75%">City block distance.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;chebychev&quot;</code></td><td width="75%">Chebychev distance (maximum coordinate
 difference).</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;minkowski&quot;</code></td><td width="75%">Minkowski distance.  The default exponent
 is 2.  To specify a different exponent, use the <code>&quot;P&quot;</code> name-value
 argument.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;mahalanobis&quot;</code></td><td width="75%">Mahalanobis distance, computed using a
 positive definite covariance matrix.  To change the value of the covariance
 matrix, use the <code>&quot;Cov&quot;</code> name-value argument.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;cosine&quot;</code></td><td width="75%">Cosine distance.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;correlation&quot;</code></td><td width="75%">One minus the sample linear correlation
 between observations (treated as sequences of values).</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;spearman&quot;</code></td><td width="75%">One minus the sample Spearman&rsquo;s rank
 correlation between observations (treated as sequences of values).</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;hamming&quot;</code></td><td width="75%">Hamming distance, which is the percentage
 of coordinates that differ.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;jaccard&quot;</code></td><td width="75%">One minus the Jaccard coefficient, which is
 the percentage of nonzero coordinates that differ.</td></tr>
<tr><td width="5%"></td><td width="20%"><var>@distfun</var></td><td width="75%">Custom distance function handle.  A distance
 function of the form <code>function <var>D2</var> = distfun (<var>XI</var>, <var>YI</var>)</code>,
 where <var>XI</var> is a <math>1&times;P</math> vector containing a single observation in
 <math>P</math>-dimensional space, <var>YI</var> is an <math>N&times;P</math> matrix containing an
 arbitrary number of observations in the same <math>P</math>-dimensional space, and
 <var>D2</var> is an <math>N&times;P</math> vector of distances, where <code>(<var>D2</var>k)</code> is
 the distance between observations <var>XI</var> and <code>(<var>YI</var>k,:)</code>.</td></tr>
</table>


 <table>
<tr><td width="18%"><code>&quot;DistanceWeight&quot;</code></td><td width="2%"></td><td width="80%">A distance weighting function,
 specified either as a function handle, which accepts a matrix of nonnegative
 distances and returns a matrix the same size containing nonnegative distance
 weights, or one of the following values: <code>&quot;equal&quot;</code>, which corresponds
 to no weighting; <code>&quot;inverse&quot;</code>, which corresponds to a weight equal to
 <math>1/distance</math>; <code>&quot;squaredinverse&quot;</code>, which corresponds to a weight
 equal to <math>1/distance^2</math>.</td></tr>
<tr><td width="18%"><code>&quot;IncludeTies&quot;</code></td><td width="2%"></td><td width="80%">A boolean flag to indicate if the
 returned values should contain the indices that have same distance as the
 <math>K^th</math> neighbor.  When <code>false</code>, <code>knnsearch</code> chooses the
 observation with the smallest index among the observations that have the same
 distance from a query point.  When <code>true</code>, <code>knnsearch</code> includes
 all nearest neighbors whose distances are equal to the <math>K^th</math> smallest
 distance in the output arguments. To specify <math>K</math>, use the <code>&quot;K&quot;</code>
 name-value pair argument.</td></tr>
<tr><td width="18%"><code>&quot;NSMethod&quot;</code></td><td width="2%"></td><td width="80%">is the nearest neighbor search method used
 by <code>knnsearch</code> as specified below.</td></tr>
</table>


 <table>
<tr><td width="5%"></td><td width="20%"><code>&quot;kdtree&quot;</code></td><td width="75%">Creates and uses a Kd-tree to find nearest
 neighbors.  <code>&quot;kdtree&quot;</code> is the default value when the number of columns
 in <var>X</var> is less than or equal to 10, <var>X</var> is not sparse, and the
 distance metric is <code>&quot;euclidean&quot;</code>, <code>&quot;cityblock&quot;</code>,
 <code>&quot;manhattan&quot;</code>, <code>&quot;chebychev&quot;</code>, or <code>&quot;minkowski&quot;</code>.  Otherwise,
 the default value is <code>&quot;exhaustive&quot;</code>.  This argument is only valid when
 the distance metric is one of the four aforementioned metrics.</td></tr>
<tr><td width="5%"></td><td width="20%"><code>&quot;exhaustive&quot;</code></td><td width="75%">Uses the exhaustive search algorithm by
 computing the distance values from all the points in <var>X</var> to each point in
 <var>Y</var>.</td></tr>
</table>


<p> <strong>See also: </strong>
  <a href="ClassificationKNN.html">ClassificationKNN</a>, 
  <a href="knnsearch.html">knnsearch</a>, 
  <a href="rangesearch.html">rangesearch</a>, 
  <a href="pdist2.html">pdist2</a>
</p>
<p><strong>Source Code: </strong>
  <a href="https://github.com/gnu-octave/statistics/tree/main/inst/fitcknn.m">fitcknn</a>
</div>

        </div>
      </div>
    </div>

  </body>
</html>
